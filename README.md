RegressÃ£o Linear Manual e Gradient Descent. 

A ideia Ã© usar as formulas matematicas de ambos mÃ©todos em vez de bibliotecas prontas.


Passos para ExecuÃ§Ã£o

1. RegressÃ£o Linear Manual
GeraÃ§Ã£o de Dados:

Eu criei um conjunto de dados sintÃ©ticos com 1000 amostras, onde a variÃ¡vel dependente Y Ã© uma funÃ§Ã£o linear de X com um pouco de ruÃ­do adicionado.

O modelo Ã© ğ‘Œ = 12.467 + 5.3 â‹… ğ‘‹ + ğœ–, onde Ïµ Ã© o ruido.

AdiÃ§Ã£o de Termo de InterceptaÃ§Ã£o:

TambÃ©m adicionei uma coluna de 1 Ã  matriz de entrada X usando a funÃ§Ã£o add_dummy_feature() da sklearn.

CÃ¡lculo dos ParÃ¢metros:
Utilizei a fÃ³rmula da regressÃ£o linear 

ğœƒ=(ğ‘‹^(ğ‘‡) @ ğ‘‹)^(âˆ’1)ğ‘‹^(ğ‘‡)ğ‘Œ 

para calcular os parÃ¢metros Î¸.

PrediÃ§Ã£o e VisualizaÃ§Ã£o:

Fiz a prediÃ§Ã£o para um novo conjunto de dados e entÃ£o visualizei os resultados, incluindo o erro quadrÃ¡tico mÃ©dio (MSE) usando a formula 

mse = (1/n)(y - Xğœƒ)^2

Resultado:

O modelo gerado tem uma boa aproximaÃ§Ã£o dos coeficientes reais e o resultado do MSE foi de 1.01.

LinearRegression:

Treinamento com LinearRegression:

Usei o modelo LinearRegression da sklearn para comparar com os meus modelos.
Treinei o modelo com o mesmo conjunto de dados e obtive um MSE de 1.0467249181812295, com interceptaÃ§Ã£o de 12.45155644 e coeficiente de 5.33025007.

2. Gradient Descent
   
InicializaÃ§Ã£o:

Similar Ã  abordagem anterior, criei os dados e adicionei o termo de interceptaÃ§Ã£o.

CÃ¡lculo via Gradient Descent:

Inicializei aleatoriamente os parÃ¢metros Î¸ e utilizei o algoritmo de Gradient Descent para reducir a funÃ§Ã£o de custo (MSE).

Coloquei a taxa de aprendizado Î· = 0.01 e o nÃºmero de iteraÃ§Ãµes (Ã©pocas) foi colocado em 1000.

Resultado:

ApÃ³s 1000 Ã©pocas, os parÃ¢metros Î¸ se aproximaram muito bem dos valores reais:

InterceptaÃ§Ã£o: 12.43

Coeficiente: 5.29


Stochastic Gradient Descent (SGD) Manual

ImplementaÃ§Ã£o Manual do SGD:

Para a implementaÃ§Ã£o manual do Stochastic Gradient Descent, inicialmente, coloquei os parÃ¢metros Î¸ como aleatorio.

A cada iteraÃ§Ã£o, peguei um valor aleatÃ³rio de X e Y, calculei o gradiente da funÃ§Ã£o de custo para essa amostra, e atualizei os parÃ¢metros Î¸.

Resultado:

ApÃ³s o treinamento, os parÃ¢metros Î¸ se aproximaram dos valores reais:

InterceptaÃ§Ã£o: 12.44

Coeficiente: 5.29


Mini-Batch Gradient Descent:

TambÃ©m usei o Mini-Batch Gradient Descent.

Dividi os dados em mini-batches e, para cada mini-batch, calculei o gradiente da funÃ§Ã£o de custo e atualizei os parÃ¢metros Î¸.
Coloquei o tamanho de cada mini-batch para 50.

Resultado:

ApÃ³s o treinamento com Mini-Batch Gradient Descent, os parÃ¢metros Î¸ convergiram para:

InterceptaÃ§Ã£o: 12.43

Coeficiente: 5.29

SGDRegressor

Treinamento com SGDRegressor:

Usei o modelo SGDRegressor da sklearn para comparar com os meus modelos.
Treinei o modelo com o mesmo conjunto de dados e obtive uma precisÃ£o de 99%, com interceptaÃ§Ã£o de 12.43 e coeficiente de 5.28.

Uso de partial_fit():

Durante o treinamento com o SGDRegressor, decidi usar o mÃ©todo partial_fit() para entender como funcionava e realizar o treinamento de forma incremental.
Este modelo obteve uma precisÃ£o de 99%.

ConclusÃµes

Parece que as funÃ§Ãµes da bibliteca sklearn e os modelos feitos de forma manual chegaram a resultados prÃ³ximos de si mesmos e dos reais.
