English description below


RegressÃ£o Linear Manual e Gradient Descent. 

A ideia Ã© usar as formulas matematicas de ambos mÃ©todos em vez de bibliotecas prontas.


Passos para ExecuÃ§Ã£o

1. RegressÃ£o Linear Manual
GeraÃ§Ã£o de Dados:

Eu criei um conjunto de dados sintÃ©ticos com 1000 amostras, onde a variÃ¡vel dependente Y Ã© uma funÃ§Ã£o linear de X com um pouco de ruÃ­do adicionado.

O modelo Ã© ğ‘Œ = 12.467 + 5.3 â‹… ğ‘‹ + ğœ–, onde Ïµ Ã© o ruido.

AdiÃ§Ã£o de Termo de InterceptaÃ§Ã£o:

TambÃ©m adicionei uma coluna de 1 Ã  matriz de entrada X usando a funÃ§Ã£o add_dummy_feature() da sklearn.

CÃ¡lculo dos ParÃ¢metros:
Utilizei a fÃ³rmula da regressÃ£o linear 

ğœƒ=(ğ‘‹^(ğ‘‡) @ ğ‘‹)^(âˆ’1)ğ‘‹^(ğ‘‡)ğ‘Œ 

para calcular os parÃ¢metros Î¸.

PrediÃ§Ã£o e VisualizaÃ§Ã£o:

Fiz a prediÃ§Ã£o para um novo conjunto de dados e entÃ£o visualizei os resultados, incluindo o erro quadrÃ¡tico mÃ©dio (MSE) usando a formula 

mse = (1/n)(y - Xğœƒ)^2

Resultado:

O modelo gerado tem uma boa aproximaÃ§Ã£o dos coeficientes reais e o resultado do MSE foi de 1.01.

LinearRegression:

Treinamento com LinearRegression:

Usei o modelo LinearRegression da sklearn para comparar com os meus modelos.
Treinei o modelo com o mesmo conjunto de dados e obtive um MSE de 1.0467249181812295, com interceptaÃ§Ã£o de 12.45155644 e coeficiente de 5.33025007.

2. Gradient Descent
   
InicializaÃ§Ã£o:

Similar Ã  abordagem anterior, criei os dados e adicionei o termo de interceptaÃ§Ã£o.

CÃ¡lculo via Gradient Descent:

Inicializei aleatoriamente os parÃ¢metros Î¸ e utilizei o algoritmo de Gradient Descent para reducir a funÃ§Ã£o de custo (MSE).

Coloquei a taxa de aprendizado Î· = 0.01 e o nÃºmero de iteraÃ§Ãµes (Ã©pocas) foi colocado em 1000.

Resultado:

ApÃ³s 1000 Ã©pocas, os parÃ¢metros Î¸ se aproximaram muito bem dos valores reais:

InterceptaÃ§Ã£o: 12.43

Coeficiente: 5.29


Stochastic Gradient Descent (SGD) Manual

ImplementaÃ§Ã£o Manual do SGD:

Para a implementaÃ§Ã£o manual do Stochastic Gradient Descent, inicialmente, coloquei os parÃ¢metros Î¸ como aleatorio.

A cada iteraÃ§Ã£o, peguei um valor aleatÃ³rio de X e Y, calculei o gradiente da funÃ§Ã£o de custo para essa amostra, e atualizei os parÃ¢metros Î¸.

Resultado:

ApÃ³s o treinamento, os parÃ¢metros Î¸ se aproximaram dos valores reais:

InterceptaÃ§Ã£o: 12.44

Coeficiente: 5.29


Mini-Batch Gradient Descent:

TambÃ©m usei o Mini-Batch Gradient Descent.

Dividi os dados em mini-batches e, para cada mini-batch, calculei o gradiente da funÃ§Ã£o de custo e atualizei os parÃ¢metros Î¸.
Coloquei o tamanho de cada mini-batch para 50.

Resultado:

ApÃ³s o treinamento com Mini-Batch Gradient Descent, os parÃ¢metros Î¸ convergiram para:

InterceptaÃ§Ã£o: 12.43

Coeficiente: 5.29

SGDRegressor

Treinamento com SGDRegressor:

Usei o modelo SGDRegressor da sklearn para comparar com os meus modelos.
Treinei o modelo com o mesmo conjunto de dados e obtive uma precisÃ£o de 99%, com interceptaÃ§Ã£o de 12.43 e coeficiente de 5.28.

Uso de partial_fit():

Durante o treinamento com o SGDRegressor, decidi usar o mÃ©todo partial_fit() para entender como funcionava e realizar o treinamento de forma incremental.
Este modelo obteve uma precisÃ£o de 99%.

ConclusÃµes

Parece que as funÃ§Ãµes da bibliteca sklearn e os modelos feitos de forma manual chegaram a resultados prÃ³ximos de si mesmos e dos reais.






Manual Linear Regression and Gradient Descent

The idea is to use the mathematical formulas for both methods instead of relying on pre-built libraries.

Steps for Execution


Manual Linear Regression

Data Generation: 

I created a synthetic dataset with 1000 samples, where the dependent variable Y is a linear function of X with some added noise. The model is 
ğ‘Œ = 12.467 + 5.3 â‹… ğ‘‹ + ğœ–, where Ïµ is the noise.

Adding Intercept Term:

I also added a column of 1s to the input matrix X using the add_dummy_feature() function from sklearn.

Parameter Calculation: 

I used the linear regression formula 

ğœƒ=(ğ‘‹^(ğ‘‡) @ ğ‘‹)^(âˆ’1)ğ‘‹^(ğ‘‡)ğ‘Œ 

to calculate the parameters Î¸.

Prediction and Visualization: 

I made predictions for a new dataset and then visualized the results, including the Mean Squared Error (MSE) using the formula:

mse = (1/n)(y - Xğœƒ)^2
 
Result: 

The generated model closely approximates the real coefficients, and the resulting MSE was 1.01.

LinearRegression:

Training with LinearRegression: 

I used the LinearRegression model from sklearn to compare it with my manual models. I trained the model with the same dataset and obtained an MSE of 1.0467249181812295, with an intercept of 12.45155644 and a coefficient of 5.33025007.

Gradient Descent

Initialization: 

Similar to the previous approach, I created the data and added the intercept term.

Calculation via Gradient Descent: 

I randomly initialized the parameters Î¸ and used the Gradient Descent algorithm to minimize the cost function (MSE). I set the learning rate 
Î·=0.01 and the number of iterations (epochs) to 1000.

Result: 

After 1000 epochs, the parameters 
Î¸ closely approximated the real values:

Intercept: 12.43

Coefficient: 5.29

Stochastic Gradient Descent (SGD) Manual

Manual SGD Implementation: 

For the manual implementation of Stochastic Gradient Descent, I initially set the parameters Î¸ randomly. For each iteration, I picked a random value of X and Y, computed the gradient of the cost function for that sample, and updated the parameters Î¸.

Result: 

After training, the parameters Î¸ approximated the real values:

Intercept: 12.44

Coefficient: 5.29

Mini-Batch Gradient Descent:

I also used Mini-Batch Gradient Descent. I divided the data into mini-batches and, for each mini-batch, calculated the gradient of the cost function and updated the parameters 
Î¸. I set the mini-batch size to 50.

Result: 

After training with Mini-Batch Gradient Descent, the parameters 
Î¸ converged to:

Intercept: 12.43

Coefficient: 5.29

SGDRegressor

Training with SGDRegressor: 

I used the SGDRegressor model from sklearn to compare with my models. I trained the model with the same dataset and obtained an accuracy of 99%, with an intercept of 12.43 and a coefficient of 5.28.

Use of partial_fit(): 

During training with SGDRegressor, I decided to use the partial_fit() method to understand how it worked and perform incremental training. This model achieved an accuracy of 99%.

Conclusions: 

It seems that the functions from the sklearn library and the manually created models yielded results that were close to each other and the real values.
